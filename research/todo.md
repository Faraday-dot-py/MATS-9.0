this is a simple todo list, it's gonna be reused a lot

- [x] Get model embeddings for layer `n`
	- [x] Get a model
		- [x] Decide which to use
			- [ ] GPT2 is small and used in a bunch of places (chose this one)
			- [ ] Start simple
			- [ ] Distilled models?
			- [ ] Needs to be a somewhat new model
		- [x] Where to download
			- [x] https://huggingface.co/openai-community/gpt2-large
	- [x] Load model
	- [x] Capture model's embeddings
		- [x] Run some text through it
- [x] Gather dataset
	- [x] What text?
		- [x] Shakespeare is easy but small
		- [ ] Open source language datasets?
		- [ ] Datasets in other languages?
	- [x] Format?
		- [ ] YES! Sentences end with punctuation
			- [x] Fix: modify the data loader/file loader to split the data at ~~random points~~ topic-specific points
				- Do still do random points in a line later
			- [ ] Fix 2: Generate a ds by making a sequential super set of the text (shakespeare atp)
- [x] Run first (small) training pass
	- [x] Does the model start learning?
- [x] Run big training pass
	- [x] Can we start to see features forming?
- [x] Tuning
	- [x] Found good baseline!
- [ ] Feature steering