this is a simple todo list, it's gonna be reused a lot

- [x] Get model embeddings for layer `n`
	- [x] Get a model
		- [x] Decide which to use
			- [ ] GPT2 is small and used in a bunch of places (chose this one)
			- [ ] Start simple
			- [ ] Distilled models?
			- [ ] Needs to be a somewhat new model
		- [x] Where to download
			- [x] https://huggingface.co/openai-community/gpt2-large
	- [x] Load model
	- [x] Capture model's embeddings
		- [x] Run some text through it
- [x] Gather dataset
	- [x] What text?
		- [x] Shakespeare is easy but small
		- [ ] Open source language datasets?
		- [ ] Datasets in other languages?
	- [x] Format?
		- [ ] YES! Sentences end with punctuation
			- [x] Fix: modify the data loader/file loader to split the data at ~~random points~~ topic-specific points
				- Do still do random points in a line later
			- [ ] Fix 2: Generate a ds by making a sequential super set of the text (shakespeare atp)
	- [ ] More data!
		- [ ] Upload data to google drive
		- [x] Connect colab to google drive dataset
- [x] Run first (small) training pass
	- [x] Does the model start learning?
- [x] Run big training pass
	- [x] Can we start to see features forming?
- [x] Tuning
	- [x] Found good baseline!
	- [x] Need to re-tune to account for more data
- [ ] Feature steering
	- [ ] Come up with probe prompts
	- [ ] Make metrics to measure how the targets move
	- [ ] Does the SAE recognize different features based on the directionality of a prompt?
- [ ] Feature analysis
	- [ ] Compare cosines of:
		- [ ] Different features on same seed
		- [ ] Same features on different seeds
		- [ ] Top features on diff seeds
		- [ ] This should give some interesting similarity results