{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "06d61f07",
      "metadata": {
        "id": "06d61f07"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as torchFun\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0f1251be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f1251be",
        "outputId": "3861ea0a-eb9f-447d-bb3e-72bc49f93fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_NAME = \"google/gemma-3-270m-it\"  # start small;\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None: # REALLY IMPORTANT - otherwise GPT-2 needs inputs of the same length\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "                                            # Output hidden states lets us see the last hidden layer\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, output_hidden_states=True).to(device).eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If I'm not using my school's google drive account, that means it's free storage!\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP1efVbbvV2B",
        "outputId": "237e3344-36c7-4a6b-c04b-50e8b865be24"
      },
      "id": "mP1efVbbvV2B",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2n47nsOb-kBn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n47nsOb-kBn",
        "outputId": "1fedc8aa-d6b4-4771-dc38-f35036ea41b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3373\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/enwiki_formatted_ds/enwiki_raw_ds/ | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "GW6dpp8Ftkx2",
      "metadata": {
        "id": "GW6dpp8Ftkx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70f97901-a81e-4fe8-8649-e040a3e19c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3373/3373 [02:18<00:00, 24.41it/s]\n"
          ]
        }
      ],
      "source": [
        "import os, glob\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from itertools import chain\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ---------- Faster file reading (parallel, IO-bound so threads work well) ----------\n",
        "def _read_one_file(fp):\n",
        "    lines = []\n",
        "    with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            s = line.strip()\n",
        "            if s:\n",
        "                lines.append(s)\n",
        "    return lines\n",
        "\n",
        "def read_all_lines_from_folder(folder=\"data\", max_workers=8):\n",
        "    files = sorted(glob.glob(os.path.join(folder, \"**/*.txt\"), recursive=True))\n",
        "    lines_per_file = []\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        for chunk in tqdm(ex.map(_read_one_file, files), total=len(files)):\n",
        "            lines_per_file.append(chunk)\n",
        "    # Flatten\n",
        "    return list(chain.from_iterable(lines_per_file))\n",
        "\n",
        "# ---------- Dataset returns raw strings; tokenize later in collate ----------\n",
        "class SimpleTextDataset(Dataset):\n",
        "    def __init__(self, strings):\n",
        "        self.strings = strings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.strings)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.strings[i]\n",
        "\n",
        "# ---------- Collate function: vectorized, batched tokenization ----------\n",
        "def make_collate_fn(tokenizer, max_len=256):\n",
        "    def collate_fn(batch_strings):\n",
        "        # batch_strings: List[str]\n",
        "        enc = tokenizer(\n",
        "            batch_strings,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=max_len,\n",
        "            padding=\"max_length\",  # keep same behavior as your original code\n",
        "        )\n",
        "        return enc\n",
        "    return collate_fn\n",
        "\n",
        "# ---------- Usage ----------\n",
        "# If you're on GPU, pinned memory + more workers helps a lot.\n",
        "pin = torch.cuda.is_available()\n",
        "num_workers = max(1, os.cpu_count() // 2)  # adjust as you like\n",
        "\n",
        "allLines = read_all_lines_from_folder(\n",
        "    \"/content/drive/MyDrive/enwiki_formatted_ds/enwiki_raw_ds\",\n",
        "    max_workers=8,  # tune based on your disk/VM\n",
        ")\n",
        "\n",
        "dataset = SimpleTextDataset(allLines)\n",
        "collate_fn = make_collate_fn(tokenizer, max_len=256)\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=False, # WHATEVER YOU DO, DO NOT SHUFFLE\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin,\n",
        "    persistent_workers=(num_workers > 0),\n",
        "    prefetch_factor=2,  # bump to 4 if you have RAM\n",
        "    collate_fn=collate_fn,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXke5MQ_ij3_",
        "outputId": "3715ed39-85c9-491c-e240-2aa110db98a7"
      },
      "id": "aXke5MQ_ij3_",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33516409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3ed539bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ed539bd",
        "outputId": "88d85d43-ce99-4518-f9c6-b2c352c680b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2500it [06:24,  6.51it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([40000, 640])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# This is where we get the interesting bits\n",
        "\n",
        "hiddenLayerToGrab = -1\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_activations(dataloader, takeLastToken=True, maxBatches=50):\n",
        "    outputActivations = [] # The eventual feature activations\n",
        "    for i, batch in tqdm(enumerate(dataloader)):\n",
        "        if i >= maxBatches: break # Make sure we don't get lost in the sauce\n",
        "        \"\"\"\n",
        "        Q: What does this next line mean?\n",
        "        A: Move all the tensors from the dataloader batches to {device}\n",
        "        \"\"\"\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        # Where the magic happens\n",
        "        # Pass the batch through the model\n",
        "        out = model(**batch)\n",
        "        hiddenStates = out.hidden_states[hiddenLayerToGrab]\n",
        "        # TODO: Uncomment this next line and pass the hidden states through a normalization function\n",
        "        # this might help with scaling artifacts (more accurate representation of what the model \"wants to say\")\n",
        "        hiddenStates = torch.nn.LayerNorm(hiddenStates.size(-1), elementwise_affine=False).to(hiddenStates.device)(hiddenStates)\n",
        "        if takeLastToken:\n",
        "            # TODO: Change the layer that we're looking at and see if there's any interesting activations there\n",
        "            # TODO: Change the token we're grabbing, as there's a high chance the last token is punctuation\n",
        "            lastHiddenState = hiddenStates[:, -1, :] # This is the last hidden state (final res stream)\n",
        "                            # High in semantic data ^\n",
        "            # TODO: Randomly break up text. The last token may be punctuation heavy\n",
        "        else:\n",
        "            lastHiddenState = hiddenStates.reshape(-1, hiddenStates.size(-1))\n",
        "\n",
        "        \"\"\"\n",
        "        Q: What does detach() do?\n",
        "        A: It pulls the tensor away from the computation graph\n",
        "        Reason: That's all we need. If we don't, PyTorch will run backprop (don't need it)\n",
        "        \"\"\"\n",
        "\n",
        "        outputActivations.append(lastHiddenState.detach().cpu())\n",
        "\n",
        "    return torch.cat(outputActivations, dim=0)\n",
        "\n",
        "activations = collect_activations(dataloader, takeLastToken=True, maxBatches=2500)\n",
        "modelDims = activations.shape[-1]\n",
        "activations.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dd3cf401",
      "metadata": {
        "id": "dd3cf401"
      },
      "outputs": [],
      "source": [
        "# Building the magic to make sense of the interesting bits\n",
        "class SAE(nn.Module):\n",
        "    def __init__(self, inDims, codeDims, tied=False, topk=None):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Linear(inDims, codeDims, bias=True) # Bias is true because it's good at learning activation offsets\n",
        "        self.decoder = nn.Linear(codeDims, inDims, bias=False) # Bias is false because we want representations to be just combinations of feature directions\n",
        "        # Note: If we had bias true, some feature vectors could be offset by an arbitrary amount,\n",
        "        # making it harder to compare veature vectors in some situations\n",
        "        self.tied = tied # True can help improve improve how identifiable features are, constrains the solution\n",
        "        self.topk = topk # Hard sparsity - keeps only largest k activations - used for fixed amount of active features\n",
        "        if tied:\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "    def encode(self, x):\n",
        "        # x is the hidden layer we're passing in\n",
        "        s = self.encoder(x)\n",
        "        if self.topk is not None:\n",
        "            # k = self.topk\n",
        "            # Grab the top k values and their dimensions\n",
        "            topkVals, topkIndex = torch.topk(s, self.topk, dim=-1)\n",
        "            # Make a mask out of them\n",
        "            mask = torch.zeros_like(s).scatter_(-1, topkIndex, 1.0)\n",
        "            # Multiply the mask in and kill any features that aren't within the {self.topk} dimensions\n",
        "            # Works as a \"hard sparsity\"\n",
        "            s = s * mask\n",
        "            s = torchFun.relu(s)\n",
        "        else:\n",
        "            # Relies on L1 penalty in loss - \"soft sparsity\"\n",
        "            s = torchFun.relu(s)\n",
        "        return s\n",
        "\n",
        "    def forward(self, x):\n",
        "        s = self.encode(x)\n",
        "        xHat = self.decoder(s)\n",
        "        return xHat, s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a75be99f",
      "metadata": {
        "id": "a75be99f"
      },
      "outputs": [],
      "source": [
        "# SAE training function\n",
        "from tqdm import trange\n",
        "\n",
        "def train_sae(X, codeDims=4*modelDims, l1Strength=1e-3, epochs=5, batchSize=256, topk=None, learningRate=1e-3, tied=False):\n",
        "    sae = SAE(inDims=modelDims, codeDims=codeDims, tied=tied, topk=topk).to(device)\n",
        "    optimizer = torch.optim.Adam(sae.parameters(), lr=learningRate)\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(X) # X is CPU tensor\n",
        "    dataLoader = DataLoader(dataset, batch_size=batchSize, shuffle=True, drop_last=True)\n",
        "\n",
        "    pbar = trange(epochs, desc='Bar desc', leave=True)\n",
        "    for ep in pbar:\n",
        "        losses, reconstructionLosses, l1Losses = [], [], []\n",
        "        for (batchActivations,) in dataLoader:\n",
        "            batchActivations = batchActivations.to(device)\n",
        "            xhat, s = sae(batchActivations)\n",
        "            reconstructionLoss = torchFun.mse_loss(xhat, batchActivations)\n",
        "            l1Penalty = s.abs().mean()\n",
        "            loss = reconstructionLoss + l1Strength * l1Penalty\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            reconstructionLosses.append(reconstructionLoss.item())\n",
        "            l1Losses.append(l1Penalty.item())\n",
        "        pbar.set_description(f\"ep {ep}: loss {sum(losses)/len(losses):.4f} | recon {sum(reconstructionLosses)/len(reconstructionLosses):.4f} | L1 {sum(l1Losses)/len(l1Losses):.4f}\")\n",
        "        pbar.refresh()\n",
        "\n",
        "    return sae.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8cc9d9e7",
      "metadata": {
        "id": "8cc9d9e7"
      },
      "outputs": [],
      "source": [
        "# A helpful function to get the topk features\n",
        "# This is what maximally activates features\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_feature_topk(sae, X, k=20, batch=1024):\n",
        "    indexes = []\n",
        "    allScores = []\n",
        "    for i in range(0, X.size(0), batch):\n",
        "        batchActivations = X[i:i+batch].to(device) # xb is a batch of activations\n",
        "        _, s = sae(batchActivations) # s is sparse code\n",
        "        allScores.append(s.detach().cpu())\n",
        "    scores = torch.cat(allScores, dim=0)\n",
        "\n",
        "    for j in range(scores.size(1)):\n",
        "        vals, topIndex = torch.topk(scores[:, j], k)\n",
        "        indexes.append(topIndex)\n",
        "    return indexes, scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0fd0f5d2",
      "metadata": {
        "id": "0fd0f5d2"
      },
      "outputs": [],
      "source": [
        "# Feature steering\n",
        "# Lets us push features in directions (small + 1 = medium + 1 = large)\n",
        "# Shows that a direction has meaning\n",
        "\n",
        "@torch.no_grad()\n",
        "def steer_with_feature(model, tokenizer, prompt, sae, featureId, epsilon=0.5):\n",
        "    # Run a pass through the LM\n",
        "    encoder = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    out = model(**encoder, output_hidden_states=True)\n",
        "    H = out.hidden_states[-1][:, -1, :]\n",
        "\n",
        "    # Run a pass through the SAE with the LM's hidden states\n",
        "    _, s = sae(H)\n",
        "    d = sae.decoder.weight[featureId]\n",
        "\n",
        "    # Steer the SAE's outputs in a direction\n",
        "    H_steered = H + epsilon * d\n",
        "\n",
        "    # Pass the logits back out for analysis\n",
        "    logits = model.lm_head(H)\n",
        "    logits_steered = model.lm_head(H_steered)\n",
        "    return logits.squeeze(0), logits_steered.squeeze(0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "Amt8NSRRhleI",
      "metadata": {
        "id": "Amt8NSRRhleI"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Some helper functions for analysis\n",
        "@torch.no_grad()\n",
        "def eval_reconstruction_stats(sae, Xeval, batch=2048):\n",
        "    reconstructionSum, elements = 0.0, 0\n",
        "    allScores = []\n",
        "    print(len(Xeval), batch)\n",
        "    for i in range(0, len(Xeval), batch):\n",
        "        xb = Xeval[i:i+batch].to(device)\n",
        "        xhat, score = sae(xb)\n",
        "        # Calculate the mean squared error of the model over the evaluation ds\n",
        "        reconstructionSum += torchFun.mse_loss(xhat, xb, reduction='sum').item()\n",
        "        elements += xb.numel()\n",
        "        allScores.append(score.detach().cpu())\n",
        "    mse = reconstructionSum / elements\n",
        "    variance = Xeval.pow(2).mean().item()\n",
        "    r2 = max(0.0, 1.0 - mse / (variance + 1e-12))\n",
        "    score = torch.cat(allScores, dim=0)\n",
        "    return mse, r2, score\n",
        "\n",
        "@torch.no_grad()\n",
        "def calc_sparsity_metrics(encoderActivations):\n",
        "    # Feature activation density\n",
        "    density = (encoderActivations > 0).float().mean().item()\n",
        "    # Part of dictionary that's unused\n",
        "    deadRate = (encoderActivations.sum(dim=0) == 0).float().mean().item()\n",
        "    # How strongly/how often a feature is used\n",
        "    usage = encoderActivations.abs().sum(dim=0)\n",
        "    # gini coefficient formula -> degree of variation in dataset\n",
        "    if usage.sum() == 0:\n",
        "        gini = 0.0\n",
        "    else:\n",
        "        u = torch.sort(usage)[0]\n",
        "        n = u.numel()\n",
        "        # normalized Gini (0 = equal usage, 1 = unequal)\n",
        "        gini = ((2 * torch.arange(1, n + 1) - n - 1).float().to(u) * u).sum() / (n * u.sum() + 1e-12)\n",
        "        gini = gini.item()\n",
        "    # Note: Look at idea.md, working session 5\n",
        "    # for tips on interpreting/fixing these v\n",
        "    return density, deadRate, gini\n",
        "\n",
        "@torch.no_grad()\n",
        "def decoder_cosine_summary(sae, sampleLimit=2000):\n",
        "    # weight = indims, codeDims\n",
        "    decoder = sae.decoder.weight.detach().cpu().T\n",
        "    dcode = decoder.size(0)\n",
        "    m = min(sampleLimit, max(0, (dcode * (dcode - 1)) // 2))\n",
        "    if m == 0:\n",
        "        return {\"mean\": float('nan'), \"p95\": float('nan'), \"max\": float('nan')}\n",
        "    pairs = set()\n",
        "    while len(pairs) < m:\n",
        "        i, j = random.randrange(dcode), random.randrange(dcode)\n",
        "        if i < j: pairs.add((i, j))\n",
        "    pairs = list(pairs)\n",
        "    A = torchFun.normalize(decoder[[i for i,_ in pairs]], dim=1)\n",
        "    B = torchFun.normalize(decoder[[j for _,j in pairs]], dim=1)\n",
        "    cos = (A * B).sum(dim=1)\n",
        "    cosSorted = torch.sort(cos).values\n",
        "    return {\n",
        "        \"mean\": cos.mean().item(),\n",
        "        \"p95\": cosSorted[int(0.95*len(cosSorted))-1].item() if len(cosSorted) > 1 else cosSorted.item(),\n",
        "        \"max\": cos.max().item()\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "VK1UWifQvHPX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK1UWifQvHPX",
        "outputId": "2dc2a307-439f-4651-eba3-53faba916960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ep 99: loss 0.0207 | recon 0.0206 | L1 0.0148: 100%|██████████| 100/100 [00:36<00:00,  2.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4000 2048\n",
            "\n",
            "=== SAE Evaluation (Validation Set) ===\n",
            "Data samples: 33516409\n",
            "Grabbed hidden layer: -1\n",
            "N_val: 4000  |  modelDims: 640  |  codeDims: 2560\n",
            "Reconstruction MSE: 0.038026\n",
            "Reconstruction R^2: 0.8218   (vs centered baseline)\n",
            "Activation density (mean L0 fraction): 0.0250\n",
            "Dead feature rate: 0.0113\n",
            "Feature-usage inequality (gini-like): 0.5222\n",
            "Decoder cosine summary: mean=-0.001, p95=0.111, max=0.317\n",
            "\n",
            "=== Feature cards (exemplar indices) ===\n",
            "\n",
            "Feature 526 — top 10 exemplar rows: [36294, 8146, 11544, 26972, 16646, 9348, 28838, 20319, 25980, 34508]\n",
            "  • Updated for entire regular season. Tiebrkrs: KEN&FLA>LSU KEN>FLA hd–hd. MISS>UGA hd–hd.\n",
            "  • The Civic Democratic Party (ODS) was the most successful party in the first round, with 11 candidates proceeding to the second round. ANO 2011 had 10 candidates proceeding.\n",
            "  • Academy for Global Exploration (AGE) is a college preparatory school traveling high school for grades 9 through 12 based in Ashland, Oregon, United States. AGE spends half of each semester overseas in\n",
            "  • The Controlled and Harmonised Aeronautical Information Network (abbreviated: CHAIN) is a concept developed by EUROCONTROL to improve the quality, integrity, accuracy and interoperability of aeronautic\n",
            "  • Since its debut at the 2010 Toronto International Film Festival, ANPO: Art X War has screened at the 2010 Vancouver International Film Festival, the 2010 Doc NYC film festival, the Minneapolis/Saint P\n",
            "  • ANDOS is a Russian operating system for Electronika BK series computers: BK-0010, BK-0011, and BK-0011M. They were based on the PDP-11 architecture by Digital Equipment Corporation. ANDOS was created \n",
            "  • The Code of Ethics was developed and written by Captain Gene Muehleisen of San Diego PD as chairman of the Professional Committee of the Peace Officers Research Association of California (PORAC).\n",
            "  • The ANU Computer Science Students Association (ANU CSSA) caters to the needs of IT, Computer Science and Software Engineering (CSIT) students. The organisation had existed since at least 1995, and was\n",
            "  • It was built for Compañía Mercantil y Ganadera S.A. (COMEGA), in a privileged area which previously had occupied the residence of Francisco Madero, then transformed into Grand National Hotel.\n",
            "  • The Dutch Amateur Radio Emergency Service (DARES), which was founded on 12 May 2004, is a non-profit organization made out of licensed radio amateurs in the Netherlands. DARES participants are able to\n",
            "\n",
            "Feature 1088 — top 10 exemplar rows: [26220, 26221, 21930, 38620, 31547, 31120, 29713, 31721, 17698, 19978]\n",
            "  • Dahoma is an unincorporated community in Nassau County, Florida, United States. It is located on US 301, in the southwestern part of the county.\n",
            "  • Dahoma is located at 30°28′11″N 81°54′31″W / 30.4697°N 81.9086°W.\n",
            "  • Doman designed the Allied Subjects' Medal which was issued in 1922. This medal design was produced for a War Office competition of 1921. He also designed the 1928 Armistice medal (marking 10 years sin\n",
            "  • Dhasmana was born on 2 October 1957 in Pauri, Uttarakhand and holds an M.Com. degree from Uttar Pradesh. He is 1981-batch Madhya Pradesh cadre IPS officer and was appointed on 3 December 1981. His dom\n",
            "  • AllThingsD.com also hosted content concerning its D Conferences; besides the annual main event in late Spring, in December 2010 they hosted D: Dive Into Mobile, the first brand extension of the confer\n",
            "  • Dagon Township (Burmese: ဒဂုံ မြို့နယ်) is located immediately north of downtown Yangon. The township comprises five wards, and shares borders with Bahan Township in the north, Ahlon Township in the w\n",
            "  • Lama Shashi Dhoj Tulachan is the spiritual leader of Chhairo Gompa having been given responsibility for Chhairo Gompa by the current incarnation who is not a practicing lama.He is responsible for the \n",
            "  • Since 2021, Darian Clark has led the organisation as Chief Executive Officer.\n",
            "  • Drayton Entertainment does not receive funding from any level of government for its annual operations and annual attendance exceeds 250,000. This makes Drayton Entertainment one of the largest profess\n",
            "  • Dzidzienyo died of cancer in October 2020. His extensive personal collection of research documents from his years at Brown was later sorted and archived by students. He formed \"lasting personal relati\n",
            "\n",
            "Feature 1114 — top 10 exemplar rows: [23560, 38106, 15817, 13550, 25034, 21975, 29454, 31966, 30027, 23382]\n",
            "  • A massage session typically begins with light petting from head to tail, both to relax the cat and to note areas of muscular tension, lumps, swelling, elevated temperature. After the cat has shown tha\n",
            "  • The depletion of Zn is necessary, because this isotope is transformed into Zn by neutron capture. Zn with a half-life of 244.26 days emits gamma radiation with 1.115 MeV. Zn has a natural abundance of\n",
            "  • In 1791, the French Academy of Sciences chose to define the metre as one ten-millionth of the distance from the equator to the North or South Pole. This replaced the earlier definition based on the pe\n",
            "  • By 1648, records indicate that there had been a noticeable extension of the spit at Shoreham of 1.4 kilometres (0.87 mi) and the total length of the spit was 2.4 kilometres (1.5 mi). A 1698 record sho\n",
            "  • This species, at 21 cm (8.3 in), is a fairly large species. Among standard measurements, the wing chord is 7.9 to 9.2 cm (3.1 to 3.6 in), the relatively short tail is 8.9 to 10.6 cm (3.5 to 4.2 in), t\n",
            "  • The height of the shell varies between 10 mm and 12 mm, its diameter between 12 mm and 15 mm. The very solid, deeply, narrowly false-umbilicate shell has a globose-conic shape. It is fawn colored, lig\n",
            "  • Corymbia ferruginea is a straggly tree that typically grows to a height of 3–12 m (9.8–39.4 ft) and forms a lignotuber. Young plants and coppice regrowth have more or less sessile, rusty green, hairy,\n",
            "  • In 1958, she began dividing her time between Barcelona, Paris, and Caracas and continued to show in group exhibitions at the Sala Mendoza (a Kunsthalle run by the Fundación Mendoza in Caracas) in 1959\n",
            "  • True to its name, the glacier lies inside the north-facing crater left by the 1980 eruption of Mount St. Helens and the glacier's elevation is about 6,794 ft (2,071 m). A massive central lava dome emp\n",
            "  • Geyish brown; the vertex of head dark; pro-, meso-, and meta- thorax each with a dark transverse streak; dorso-lateral oblique dark stripes on each segment of the abdomen. Fore wing with nine curved a\n",
            "\n",
            "Feature 1336 — top 10 exemplar rows: [20922, 30905, 19317, 24205, 28134, 28049, 26675, 38827, 38328, 27143]\n",
            "  • Male, female. Forewing length 3.7 mm. Head: frons ochreous-white, vertex, neck tufts and collar dark brown with reddish gloss, laterally and medially lined white; labial palpus first segment very shor\n",
            "  • His first work, written in conjunction with Charles-Louis-Joseph-Xavier de la Vallée-Poussin (1827–1904), was the Mémoire sur les caractères minéralogiques et stratigraphiques des roches dues plutonie\n",
            "  • The spreading prostrate shrub typically grows to a height of 0.05 to 0.5 metres (0 to 2 ft) with a sprawling habit. The multistemmed shrub has quite slender branches with a length of up to 1 m (3 ft 3\n",
            "  • All such courts hear trials in at least three divisions (wydziały): a civil (cywilny), a criminal (karny) and a family and minors (rodzinny i nieletnich) one. In addition to that, the courts based in \n",
            "  • Male. Forewing length 3.8 mm. Head: frons shining ochreous-white with greenish and reddish reflections, vertex and neck tufts dark bronze brown with reddish gloss, laterally and medially lined white, \n",
            "  • The Putative Archaeal 2 TMS Holin (A2-Hol) Family (TC# 9.B.109) consists of a few putative holins from Nitrososphaerota ranging in size from about 130 to 165 amino acyl residues (aas) and exhibiting 2\n",
            "  • The first fossils of Cristatusaurus were found in 1973 by French paleontologist Philippe Taquet at Gadoufaoua, a locality within the Elrhaz Formation in Niger. The holotype specimen, cataloged under t\n",
            "  • Mourad Rais is a Project 1159T frigate (NATO reporting name: Koni II). The frigate has a length of 96.4 m (316 ft), beam of 12.6 m (41 ft) and draught of 3.5 m (11 ft). The frigate has a displacement \n",
            "  • The main island of Abaiang, Teiro (not to be confused with the small islet of Teirio) has a total land area of 3,552.6 acres (14.377 km) extends from the northern village of Takarano to the southern v\n",
            "  • There are three churches in the colonia with the most important being the San Cosme Church, which was founded in 1575 by Juan de Zumárraga over the ruins of an earlier hermitage dedicated to Saints Co\n",
            "\n",
            "Feature 28 — top 10 exemplar rows: [3326, 18718, 31414, 33848, 39373, 3009, 10759, 8036, 5328, 3008]\n",
            "  • 2,4,5-Trichlorophenol (TCP) is an organochloride with the molecular formula C H Cl O. It has been used as a fungicide and herbicide. Precursor chemical used in the production of 2,4,5-Trichlorophenoxy\n",
            "  • Acyl azides have also been synthesized from various carboxylic acids and sodium azide in presence of triphenylphosphine and trichloroacetonitrile catalysts in excellent yields at mild conditions. Anot\n",
            "  • It is also possible to synthesize a target DNA strand for a DNA construct. Short strands of DNA known as oligonucleotides can be developed using column-based synthesis, in which bases are added one at\n",
            "  • Sliding clamps are loaded onto their associated DNA template strands by specialized proteins known as \" sliding clamp loaders \", which also disassemble the clamps after replication has completed. The \n",
            "  • Due to its large size and limited availability, the organic chemistry of dicoronylene is little known. Dicoronylene does undergo a Diels–Alder reaction with maleic anhydride on one or both of the cent\n",
            "  • This dione condenses with malononitrile to give an intermediate that can be dehydrogenated to tetracyanoquinodimethane (TCNQ).\n",
            "  • Acyl-protein thioesterases feature 3 major structural components that determine protein function and substrate processing: 1. A conserved, classical catalytic triad to break ester and thioester bonds;\n",
            "  • This reaction mechanism has been extensively studied in reactions of metal amide nucleophiles (such as sodium amide) and substituted pyrimidines (for instance 4-phenyl-6-bromopyrimidine 1) in ammonia \n",
            "  • 2,3-sigmatropic rearrangements can offer high stereoselectivity. At the newly formed double bond there is a strong preference for formation of the E- alkene or trans isomer product. The stereochemistr\n",
            "  • 1,4-Cyclohexanedione is prepared in two steps from diesters of succinic acid. Specifically under basic conditions, the diethyl succinate condenses to give the cyclohexenediol derivative diethylsuccino\n",
            "\n",
            "Approx mean active features per sample (val): 64.0\n",
            "Model saved to savedModels/model_4_1757705426.3403814.pth\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate SAE\n",
        "import math, random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as torchFun\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "codeDimsMultiplier = 4      # A multiplier for the \"number of concepts\" a model can learn\n",
        "l1Strength         = 1e-3   # The strength of the l1 \"lens\"\n",
        "epochs             = 100    # Iterations to train for\n",
        "batchSize          = 512    # Samples to process at the same time\n",
        "topkFeatures       = 64    # Number of top features to keep\n",
        "learningRate       = 1e-3   # Learning rate to iterate at\n",
        "seed               = 4738   # Random number seed\n",
        "printTopN          = 10      # How many examples to print\n",
        "showFeatures       = 5      # Number of features to show\n",
        "saveTopFeatures    = True   # Whether or not to save the top feature texts\n",
        "\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "X_raw = activations.clone()\n",
        "with torch.no_grad():\n",
        "    X_mean = X_raw.mean(dim=0, keepdim=True)\n",
        "X = (X_raw - X_mean)\n",
        "N, d_in = X.shape\n",
        "assert d_in == modelDims, f\"Expected d_in == modelDims, got {d_in} vs {modelDims}\"\n",
        "codeDims = codeDimsMultiplier * modelDims\n",
        "\n",
        "perm = torch.randperm(N)\n",
        "valFrac = 0.1\n",
        "nVal = max(1, int(N * valFrac))\n",
        "valIdx, trainIdx = perm[:nVal], perm[nVal:]\n",
        "X_train, X_val = X[trainIdx], X[valIdx]\n",
        "\n",
        "# for topkFeatures in [32, 64, 96]:\n",
        "if True: #This is dummy so I don't have to indent everything below\n",
        "  sae = train_sae(\n",
        "      X_train,\n",
        "      codeDims=codeDims,\n",
        "      l1Strength=l1Strength,\n",
        "      epochs=epochs,\n",
        "      batchSize=batchSize,\n",
        "      topk=topkFeatures,\n",
        "      learningRate=learningRate,\n",
        "      tied=False,\n",
        "  )\n",
        "\n",
        "\n",
        "  mseVal, r2Val, S_val = eval_reconstruction_stats(sae, X_val)\n",
        "  density, deadRate, gini = calc_sparsity_metrics(S_val)\n",
        "  cosStats = decoder_cosine_summary(sae)\n",
        "\n",
        "  print(\"\\n=== SAE Evaluation (Validation Set) ===\")\n",
        "  print(f\"Data samples: {len(dataset)}\")\n",
        "  print(f\"Grabbed hidden layer: {hiddenLayerToGrab}\")\n",
        "  print(f\"N_val: {X_val.size(0)}  |  modelDims: {modelDims}  |  codeDims: {codeDims}\")\n",
        "  print(f\"Reconstruction MSE: {mseVal:.6f}\")\n",
        "  print(f\"Reconstruction R^2: {r2Val:.4f}   (vs centered baseline)\")\n",
        "  print(f\"Activation density (mean L0 fraction): {density:.4f}\")\n",
        "  print(f\"Dead feature rate: {deadRate:.4f}\")\n",
        "  print(f\"Feature-usage inequality (gini-like): {gini:.4f}\")\n",
        "  print(f\"Decoder cosine summary: mean={cosStats['mean']:.3f}, p95={cosStats['p95']:.3f}, max={cosStats['max']:.3f}\")\n",
        "\n",
        "\n",
        "  if 'get_feature_topk' in globals():\n",
        "      idxs, S_all = get_feature_topk(sae, X)\n",
        "      featActivity = S_val.abs().sum(dim=0)\n",
        "      kShow = min(showFeatures, featActivity.numel())\n",
        "      topFeatIds = torch.topk(featActivity, k=kShow).indices.tolist()\n",
        "      saveNumber = len(os.listdir('feature_text_output'))\n",
        "\n",
        "      print(\"\\n=== Feature cards (exemplar indices) ===\")\n",
        "      for j in topFeatIds:\n",
        "          topIdx = idxs[j][:printTopN].tolist()\n",
        "          print(f\"\\nFeature {j} — top {printTopN} exemplar rows: {topIdx}\")\n",
        "          try:\n",
        "              if 'dataset' in globals() and hasattr(dataset, 'strings'):\n",
        "                  for r in topIdx:\n",
        "                      if 0 <= r < len(dataset.strings):\n",
        "                          print(\"  •\", dataset.strings[r][:200].replace(\"\\n\", \" \"))\n",
        "                          if saveTopFeatures:\n",
        "                              if not os.path.exists(f'feature_text_output'):\n",
        "                                  os.mkdir('feature_text_output')\n",
        "                              with open(f'feature_text_output/save_{saveNumber}_feature_{j}.txt', 'a') as f:\n",
        "                                  f.write(dataset.strings[r].replace(\"\\n\", \" \") + \"\\n\")\n",
        "          except Exception as e:\n",
        "              print(e)\n",
        "              pass\n",
        "\n",
        "  with torch.no_grad():\n",
        "      meanActive = (S_val > 0).float().sum(dim=1).float().mean().item()\n",
        "  print(f\"\\nApprox mean active features per sample (val): {meanActive:.1f}\")\n",
        "\n",
        "  # Save the model for later loading\n",
        "\n",
        "  import time\n",
        "  if not os.path.exists('savedModels'):\n",
        "      os.mkdir('savedModels')\n",
        "\n",
        "  modelPath = os.path.join('savedModels', f'model_{len(os.listdir(\"savedModels\"))}_{time.time()}.pth')\n",
        "  torch.save(sae.state_dict(), modelPath)\n",
        "  print(f\"Model saved to {modelPath}\")\n",
        "\n",
        "  varsToWrite = [\n",
        "      modelPath,\n",
        "      codeDimsMultiplier,\n",
        "      l1Strength,\n",
        "      epochs,\n",
        "      batchSize,\n",
        "      topkFeatures,\n",
        "      learningRate,\n",
        "      len(dataset),\n",
        "      hiddenLayerToGrab,\n",
        "      modelDims,\n",
        "      codeDims,\n",
        "      mseVal,\n",
        "      r2Val,\n",
        "      density,\n",
        "      deadRate,\n",
        "      gini,\n",
        "      cosStats['mean'],\n",
        "      cosStats['p95'],\n",
        "      cosStats['max'],\n",
        "      meanActive,\n",
        "  ]\n",
        "\n",
        "  # Convert everything to string before joining\n",
        "  stringToWrite = \",\".join(str(v) for v in varsToWrite) + \"\\n\"\n",
        "\n",
        "  # Use 'w' to overwrite or 'a' to append\n",
        "  with open('out.txt', 'a') as f:\n",
        "      f.write(stringToWrite)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputString = \"Corymbia ferriticola\"\n",
        "\n",
        "enc = tokenizer(\n",
        "    inputString,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    max_length=256,\n",
        "    padding=\"max_length\",\n",
        ").to(device)\n",
        "\n",
        "out = model(**enc, output_hidden_states=True)\n",
        "\n",
        "# pick the layer\n",
        "layer_hidden = out.hidden_states[hiddenLayerToGrab]  # (batch, seq_len, hidden_dim)\n",
        "\n",
        "# take last token’s hidden state\n",
        "last_token_hidden = layer_hidden[:, -1, :]           # (batch, hidden_dim)\n",
        "\n",
        "# move to CPU\n",
        "outputActivations = last_token_hidden.detach().cpu()\n",
        "\n",
        "# Get sparse code for the single input\n",
        "with torch.no_grad():\n",
        "    xhat, s = sae(outputActivations.to(device)) # move to device before passing to sae\n",
        "\n",
        "# Get top k activations and their indices for the single input\n",
        "k = 10 # You can change k to see more or fewer top features\n",
        "topkVals, topkIndex = torch.topk(s.squeeze(0), k)\n",
        "\n",
        "print(f\"Top {k} activating features and their values for the input string:\")\n",
        "for i in range(k):\n",
        "    print(f\"Feature Index: {topkIndex[i].item()}, Activation Value: {topkVals[i].item():.4f}\")\n",
        "\n",
        "print(\"\\nExamples of text that maximally activate these features:\")\n",
        "\n",
        "# Get top k examples for each of the top features from the full dataset\n",
        "# Use the existing get_feature_topk function on the full X dataset (activations from all text)\n",
        "idxs, S_all = get_feature_topk(sae, X) # X is the full dataset activations\n",
        "\n",
        "showExamplesPerFeature = 5 # How many examples to show per feature\n",
        "\n",
        "for j in topkIndex.tolist(): # Iterate through the indices of the top features\n",
        "    topIdx = idxs[j][:showExamplesPerFeature].tolist()\n",
        "    print(f\"\\nFeature {j} — top {showExamplesPerFeature} exemplar rows:\")\n",
        "    try:\n",
        "        if 'dataset' in globals() and hasattr(dataset, 'strings'):\n",
        "            for r in topIdx:\n",
        "                if 0 <= r < len(dataset.strings):\n",
        "                    print(\"  •\", dataset.strings[r][:200].replace(\"\\n\", \" \")) # Print up to 200 characters\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF8doPcLzuN6",
        "outputId": "0fb5e3b6-cd07-438d-90b1-f3621373035e"
      },
      "id": "qF8doPcLzuN6",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 activating features and their values for the input string:\n",
            "Feature Index: 998, Activation Value: 37.7803\n",
            "Feature Index: 2402, Activation Value: 36.5823\n",
            "Feature Index: 171, Activation Value: 33.5256\n",
            "Feature Index: 2419, Activation Value: 32.9131\n",
            "Feature Index: 2020, Activation Value: 31.1954\n",
            "Feature Index: 1370, Activation Value: 31.1126\n",
            "Feature Index: 945, Activation Value: 30.8929\n",
            "Feature Index: 182, Activation Value: 30.7989\n",
            "Feature Index: 1158, Activation Value: 30.6811\n",
            "Feature Index: 1658, Activation Value: 30.3844\n",
            "\n",
            "Examples of text that maximally activate these features:\n",
            "\n",
            "Feature 998 — top 5 exemplar rows:\n",
            "  • Eggers' work was published by prestigious publisher and thus raised also larger critical reactions of central European historians and archaeologists. Herwig Wolfram, Director of the Austrian Institute\n",
            "  • Sheep farming dominates the Dale, with some dairy farming occurring on the lower slopes towards Austwick. Austwick Beck used to have what was known locally as a Washdub. The farmers would get together\n",
            "  • The first diagnosis took place in a field of Normandy, Surrey; the second was three days later in a cattle-rotation field of a farm in Elstead, and the following day a third infection was found nearby\n",
            "  • Agricultural liberalisation speeded up after the elimination of the government procurement system of the main agricultural crops such as rice, pulses, sugarcane, cotton, etc., in 2003–04. The state al\n",
            "  • It lies in the valley of the same name in northern Minangkabau Highlands, producing rubber, coffee, sugarcane, yams, maize, tobacco and bananas and using a high amount of pesticides. The area is poor,\n",
            "\n",
            "Feature 2402 — top 5 exemplar rows:\n",
            "  • Celia's uncle, Tío Rodrigo, had arrived at the school where Celia attended class with the nuns and had rudely taken her away, without her parents permission or consent, because he strongly believed th\n",
            "  • Abyss & Apex was founded by Carol Burrell, and published by Burrell under the ByrenLee Press imprint, Riverdale, New York. Elizabeth Bear was managing editor from the first issue (January/February 200\n",
            "  • Amanda Barker was born in Hanover, Massachusetts to Daniel Barker (a drummer and cymbal maker) and Valerie Gibson Barker (a dance and performing arts teacher). When she was a teen, her father became C\n",
            "  • The church and monastery saw a number of historic events in its time. A funeral mass for Hernán Cortés was here when it was thought that he died in Central America. In 1629, the Marquis of Gelves arri\n",
            "  • Analogy was a German and Italian psychedelic rock, progressive rock band, active in the 1970s. The band was launched by the guitarist Martin Thurn when attending the European School, Varese. In 1968, \n",
            "\n",
            "Feature 171 — top 5 exemplar rows:\n",
            "  • Indirect presidential elections were held in Finland in 1946. In 1944 the Parliament had passed a law that enabled Carl Gustaf Emil Mannerheim to serve a six-year term. However, he resigned on 4 March\n",
            "  • SERVICE.--Springfield, Mo., August 9, 1862. Blount's Campaign in Missouri and Arkansas October 17-December 27. Battle of Prairie Grove, Ark., December 7. Expedition from Fayetteville to Huntsville Dec\n",
            "  • In the early 1980s, Wilson further disturbed and refined traditional anthropological thinking by his work with PhD students Rebecca Cann and Mark Stoneking on the so-called \"Mitochondrial Eve\" hypothe\n",
            "  • In The Playboy Winner's Guide to Board Games, game designer Jon Freeman noted that Cobra used the same game system as SPI's previous publication, Panzergruppe Guderian, but thought that Cobra was bett\n",
            "  • When intraspecific competition is not at work disruptive selection can still lead to sympatric speciation and it does this through maintaining polymorphisms. Once the polymorphisms are maintained in t\n",
            "\n",
            "Feature 2419 — top 5 exemplar rows:\n",
            "  • Rafał Majka, a \"key lieutenant\" of Pogačar did not start stage 17, due to an injury suffered after he threw his chain near the end of stage 16. There was a long breakaway fight as several riders tried\n",
            "  • 1985 - W.G. Howeth Jr.\n",
            "  • On 13 October 1974, Summerhill won their first Meath S.F.C. title when they defeated Bohermeen 0–9 to 0–7 in the final in Pairc Tailteann. Austin Lyons raised the Keegan Cup for the 'Hill.\n",
            "  • 2004 - Armando Soto Sr.\n",
            "  • 2006 California Boxing Hall of Fame Inductee.\n",
            "\n",
            "Feature 2020 — top 5 exemplar rows:\n",
            "  • 9 RQR's lineage can be traced through the following units: 1867–1879: The Spring Hill and Fortitude Valley Rifle Corps 1879–1885: 1st Queensland (Moreton) Regiment 1885–1903: 1st Queenslanders (The Mo\n",
            "  • At Indianapolis until December 23. Left Indiana for Kentucky December 23 and duty at Bardstown and Lebanon, Kentucky, until February 1862. March through central Kentucky to Nashville, Tennessee, Febru\n",
            "  • The tree typically grows to over 7 m (23 ft) to a maximum height of 20 m (66 ft) and has slender, brittle and pendulous branchlets with caducous and deltate stipules that have a length that is mostly \n",
            "  • The story revolves around a retired government officer, his family, his house, and also the tenants of the house. who slowly starts to realise that he has a lot on his hand after his retirement to wri\n",
            "  • In New York, Murr assisted in the design and restoration of Saint Joseph Roman Catholic Church (German National Parish), Yorkville (Manhattan), New York, New York; the remodeling of Our Lady of Guadal\n",
            "\n",
            "Feature 1370 — top 5 exemplar rows:\n",
            "  • indicates seat up for re-election. indicates councillor defection.\n",
            "  • ʔaː=tʼo béda=ht̪ow béː=yo-w dá-ːʔ-du-w tʃʰó-w. béda ʔaː qʼlá-w=ʔkʰe.\n",
            "  • As of 2018, of the current heated tobacco products, IQOS was launched in several cities in Japan, Italy and Switzerland in 2014, iFuse was released in Romania in 2015 and glo and Ploom Tech were intro\n",
            "  • According to the District Census Handbook 2011, Chatra, Chatra (nagar parishad) covered an area of 16 km. Among the civic amenities, it had 52 km roads with open drains, the protected water supply inv\n",
            "  • The bookselling profession has passed from father to son in the al-Maaytah family for four generations since the Ottoman period. The bookstore's physical location has moved dozens of times due to the \n",
            "\n",
            "Feature 945 — top 5 exemplar rows:\n",
            "  • He taught English for a year, then changed to history, which he taught at Yale from 1933, becoming an assistant professor in 1938, an associate professor in 1942, and a full professor in 1947. Griswol\n",
            "  • It has been suggested that this was Atara of the list of Thothmes III.\n",
            "  • More recently, a new wave of researchers has produced work focusing on the experience of individuals, gender history and other areas. It is flourishing as an area of medieval research, and has in Rich\n",
            "  • Following in the footsteps of his elder brothers Frank McCourt and Malachy McCourt, Alphie had his own memoir A Long Stone's Throw published in 2008. The book was well received. He had published artic\n",
            "  • Amethé Smeaton was born in Rangoon, Burma, around 1896, the daughter of Donald Mackenzie Smeaton, a British official in Burma and later a member of Parliament for the Liberal Party. She attended Girto\n",
            "\n",
            "Feature 182 — top 5 exemplar rows:\n",
            "  • 1975 - O.D. Cresswell Jr.\n",
            "  • 1985 - W.G. Howeth Jr.\n",
            "  • 2014 - Max M. Charleswell Jr.\n",
            "  • 1984 - George H. Trumbo Jr.\n",
            "  • 2004 - Armando Soto Sr.\n",
            "\n",
            "Feature 1158 — top 5 exemplar rows:\n",
            "  • Corymbia flavescens is a tree that typically grows to a height of 15 m (49 ft) and forms a lignotuber. It has smooth, powdery bark that is bright white when new and is shed in thin, greyish scales. Yo\n",
            "  • This species, at 21 cm (8.3 in), is a fairly large species. Among standard measurements, the wing chord is 7.9 to 9.2 cm (3.1 to 3.6 in), the relatively short tail is 8.9 to 10.6 cm (3.5 to 4.2 in), t\n",
            "  • Acherontiscus is an extinct genus of stegocephalians that lived in the Early Carboniferous (Mississippian era) of Scotland. The type and only species is Acherontiscus caledoniae, named by paleontologi\n",
            "  • The first fossils of Cristatusaurus were found in 1973 by French paleontologist Philippe Taquet at Gadoufaoua, a locality within the Elrhaz Formation in Niger. The holotype specimen, cataloged under t\n",
            "  • This genus markedly differs from Echinodorus by a typical paniculate inflorescence shaped as a regular pyramid. Flowering stalk is 40 – 50 cm tall, inflorescence up to 12 – 20 cm long, flowers arrange\n",
            "\n",
            "Feature 1658 — top 5 exemplar rows:\n",
            "  • When Braxton debuted \"A Change in Me\" during her opening night at the Palace Theatre on September 9, 1998, critics and audiences immediately praised the addition of the new song and Braxton's performa\n",
            "  • On March 30, 2001 they released their first full album 21st Century. The same year, Kyoka announced his first solo mini-album M.\n",
            "  • After gaining its independence in 1917 and after the Finnish Civil War of 1918, large numbers of Model 1891 Mosin–Nagant rifles were in the hands of the Finnish military. As the old barrels were worn \n",
            "  • In reality, the discoveries were ingeniously fabricated by Inghirami, inspired by the forger of Etruscan antiquities Annio da Viterbo (1437-1502). A debate regarding their authenticity, involving scho\n",
            "  • Credits adapted from Melon.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}