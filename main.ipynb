{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d61f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as torchFun\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1251be",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"gpt2\"  # start small; later swap to your target model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None: # REALLY IMPORTANT - otherwise GPT-2 needs inputs of the same length\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "                                            # Output hidden states lets us see the last hidden layer\n",
    "model = AutoModeltorchtorchFununorCausalLM.from_pretrained(MODEL_NAME, output_hidden_states=True).to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6845be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 5330k  100 5330k    0     0  17.0M      0 --:--:-- --:--:-- --:--:-- 17.1M\n"
     ]
    }
   ],
   "source": [
    "!curl \"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\" > \"shakespeare.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37fbfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextDataset(Dataset):\n",
    "    def __init__(self, strings, tokenizer, maxLen=256):\n",
    "        self.strings = strings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.maxLen = maxLen\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.strings)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        encoder = self.tokenizer(\n",
    "            self.strings[i], \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=self.maxLen,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        return {k: v.squeeze(0) for k, v in encoder.items()}\n",
    "    \n",
    "with open(\"shakespeare.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "marker=\"\"\"1609\n",
    "\n",
    "THE SONNETS\n",
    "\n",
    "by William Shakespeare\"\"\"\n",
    "\n",
    "# This nasty little one liner gets rid of the header and gives us just the text\n",
    "# Might want to get rid of the passage numbers later\n",
    "shakespeare = text[text.find(marker)+len(marker):].strip().split('\\n\\n\\n')\n",
    "\n",
    "dataset = SimpleTextDataset(shakespeare, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size = 8, shuffle = True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed539bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is where we get the interesting bits\n",
    "@torch.no_grad()\n",
    "def collect_activations(dataloader, takeLastToken=True, maxBatches=50):\n",
    "    outputActivations = [] # The eventual feature activations\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= maxBatches: break # Make sure we don't get lost in the sauce\n",
    "        \"\"\"\n",
    "        Q: What does this next line mean?\n",
    "        A: Move all the tensors from the dataloader batches to {device}\n",
    "        \"\"\"\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        # Where the magic happens\n",
    "        # Pass the batch through the model\n",
    "        out = model(**batch)\n",
    "        hiddenStates = out.hidden_states[-1]\n",
    "        # TODO: Uncomment this next line and pass the hidden states through a normalization function\n",
    "        # this might help with scaling artifacts (more accurate representation of what the model \"wants to say\")\n",
    "        # model.transformer.ln_f(hiddenStates)\n",
    "        if takeLastToken:\n",
    "            # TODO: Change the layer that we're looking at and see if there's any interesting activations there\n",
    "            # TODO: Change the token we're grabbing, as there's a high chance the last token is punctuation\n",
    "            lastHiddenState = hiddenStates[:, -1, :] # This is the last hidden state (final res stream)\n",
    "                            # High in semantic data ^\n",
    "            # TODO: Randomly break up text. The last token may be punctuation heavy\n",
    "        else:\n",
    "            lastHiddenState = hiddenStates.reshape(-1, hiddenStates.size(-1))\n",
    "        \n",
    "        \"\"\"\n",
    "        Q: What does detach() do?\n",
    "        A: It pulls the tensor away from the computation graph\n",
    "        Reason: That's all we need. If we don't, PyTorch will run backprop (don't need it)\n",
    "        \"\"\"\n",
    "        outputActivations.append(lastHiddenState.detach().cpu())\n",
    "        \n",
    "    return torch.cat(outputActivations, dim=0)\n",
    "\n",
    "activations = collect_activations(dataloader, takeLastToken=True, maxBatches=50)\n",
    "modelDims = activations.shape[-1]\n",
    "activations.shape        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8086a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.sort(\n",
      "values=tensor([-4.7111e+01, -1.1663e+01, -4.3027e+00, -3.4850e+00, -3.2420e+00,\n",
      "        -2.2635e+00, -2.1803e+00, -1.5783e+00, -1.4539e+00, -1.2617e+00,\n",
      "        -1.1979e+00, -1.1551e+00, -1.1412e+00, -1.1204e+00, -1.1139e+00,\n",
      "        -1.1133e+00, -1.0555e+00, -1.0408e+00, -1.0303e+00, -1.0244e+00,\n",
      "        -9.9003e-01, -9.7122e-01, -8.9887e-01, -8.9735e-01, -8.8653e-01,\n",
      "        -7.7512e-01, -7.4688e-01, -7.4306e-01, -7.4169e-01, -7.3494e-01,\n",
      "        -6.9814e-01, -6.9589e-01, -6.9311e-01, -6.8580e-01, -6.7909e-01,\n",
      "        -6.6439e-01, -6.6210e-01, -6.5284e-01, -6.4287e-01, -6.2844e-01,\n",
      "        -6.2577e-01, -6.1528e-01, -6.1499e-01, -6.1081e-01, -6.0931e-01,\n",
      "        -5.9798e-01, -5.8920e-01, -5.8778e-01, -5.8491e-01, -5.7865e-01,\n",
      "        -5.7658e-01, -5.6534e-01, -5.5774e-01, -5.5527e-01, -5.4867e-01,\n",
      "        -5.4848e-01, -5.4659e-01, -5.3761e-01, -5.2946e-01, -5.2832e-01,\n",
      "        -5.2809e-01, -5.2364e-01, -5.2114e-01, -5.1688e-01, -5.1488e-01,\n",
      "        -5.0916e-01, -5.0515e-01, -5.0443e-01, -4.8876e-01, -4.8477e-01,\n",
      "        -4.8048e-01, -4.7511e-01, -4.6905e-01, -4.6812e-01, -4.6749e-01,\n",
      "        -4.6693e-01, -4.6670e-01, -4.6464e-01, -4.6101e-01, -4.5300e-01,\n",
      "        -4.5248e-01, -4.5049e-01, -4.5001e-01, -4.4024e-01, -4.2768e-01,\n",
      "        -4.2003e-01, -4.1810e-01, -4.1610e-01, -4.1540e-01, -4.1449e-01,\n",
      "        -4.1437e-01, -4.1294e-01, -4.1214e-01, -4.1056e-01, -4.0710e-01,\n",
      "        -4.0710e-01, -4.0389e-01, -3.9976e-01, -3.9881e-01, -3.9750e-01,\n",
      "        -3.9559e-01, -3.9514e-01, -3.9110e-01, -3.8756e-01, -3.8693e-01,\n",
      "        -3.8666e-01, -3.8658e-01, -3.8504e-01, -3.8451e-01, -3.7847e-01,\n",
      "        -3.7736e-01, -3.7401e-01, -3.7384e-01, -3.6913e-01, -3.6658e-01,\n",
      "        -3.5903e-01, -3.5798e-01, -3.5631e-01, -3.5541e-01, -3.5357e-01,\n",
      "        -3.5135e-01, -3.4991e-01, -3.4792e-01, -3.4565e-01, -3.4380e-01,\n",
      "        -3.4249e-01, -3.3855e-01, -3.3563e-01, -3.3257e-01, -3.3247e-01,\n",
      "        -3.2955e-01, -3.2563e-01, -3.2442e-01, -3.2385e-01, -3.2180e-01,\n",
      "        -3.2143e-01, -3.1998e-01, -3.1943e-01, -3.1813e-01, -3.1757e-01,\n",
      "        -3.1350e-01, -3.1135e-01, -3.1017e-01, -3.0368e-01, -3.0359e-01,\n",
      "        -2.9946e-01, -2.9670e-01, -2.9338e-01, -2.8509e-01, -2.8352e-01,\n",
      "        -2.8143e-01, -2.7506e-01, -2.7371e-01, -2.7202e-01, -2.7118e-01,\n",
      "        -2.6862e-01, -2.6799e-01, -2.6748e-01, -2.6729e-01, -2.6057e-01,\n",
      "        -2.5913e-01, -2.5880e-01, -2.5871e-01, -2.5783e-01, -2.5731e-01,\n",
      "        -2.5704e-01, -2.5541e-01, -2.5446e-01, -2.5437e-01, -2.5408e-01,\n",
      "        -2.5381e-01, -2.5349e-01, -2.5228e-01, -2.5063e-01, -2.5055e-01,\n",
      "        -2.4763e-01, -2.4571e-01, -2.4526e-01, -2.4278e-01, -2.4210e-01,\n",
      "        -2.4160e-01, -2.4009e-01, -2.3761e-01, -2.3671e-01, -2.3621e-01,\n",
      "        -2.3422e-01, -2.3305e-01, -2.3263e-01, -2.3151e-01, -2.3108e-01,\n",
      "        -2.2522e-01, -2.2501e-01, -2.2331e-01, -2.2312e-01, -2.2194e-01,\n",
      "        -2.2122e-01, -2.1993e-01, -2.1953e-01, -2.1906e-01, -2.1849e-01,\n",
      "        -2.1543e-01, -2.1307e-01, -2.1087e-01, -2.0970e-01, -2.0810e-01,\n",
      "        -2.0276e-01, -2.0275e-01, -2.0160e-01, -2.0020e-01, -1.9752e-01,\n",
      "        -1.9698e-01, -1.9673e-01, -1.9574e-01, -1.9395e-01, -1.9019e-01,\n",
      "        -1.8881e-01, -1.8793e-01, -1.8741e-01, -1.8739e-01, -1.8387e-01,\n",
      "        -1.8348e-01, -1.8304e-01, -1.8062e-01, -1.7911e-01, -1.7609e-01,\n",
      "        -1.7532e-01, -1.7311e-01, -1.7284e-01, -1.7186e-01, -1.6786e-01,\n",
      "        -1.6722e-01, -1.6715e-01, -1.6663e-01, -1.6649e-01, -1.6231e-01,\n",
      "        -1.6168e-01, -1.5987e-01, -1.5832e-01, -1.5793e-01, -1.5745e-01,\n",
      "        -1.5708e-01, -1.5660e-01, -1.5386e-01, -1.5370e-01, -1.5359e-01,\n",
      "        -1.5254e-01, -1.5203e-01, -1.5124e-01, -1.4980e-01, -1.4962e-01,\n",
      "        -1.4573e-01, -1.4511e-01, -1.4423e-01, -1.4375e-01, -1.4054e-01,\n",
      "        -1.4001e-01, -1.3874e-01, -1.3856e-01, -1.3845e-01, -1.3606e-01,\n",
      "        -1.3562e-01, -1.3535e-01, -1.3311e-01, -1.3261e-01, -1.3117e-01,\n",
      "        -1.2763e-01, -1.2741e-01, -1.2557e-01, -1.2455e-01, -1.2251e-01,\n",
      "        -1.2226e-01, -1.2091e-01, -1.1936e-01, -1.1866e-01, -1.1793e-01,\n",
      "        -1.1768e-01, -1.1679e-01, -1.1574e-01, -1.1364e-01, -1.1352e-01,\n",
      "        -1.1248e-01, -1.1230e-01, -1.1217e-01, -1.1189e-01, -1.1149e-01,\n",
      "        -1.1029e-01, -1.1012e-01, -1.0802e-01, -1.0786e-01, -1.0636e-01,\n",
      "        -1.0457e-01, -1.0259e-01, -1.0140e-01, -1.0103e-01, -9.7794e-02,\n",
      "        -9.6890e-02, -9.6487e-02, -9.0744e-02, -9.0497e-02, -8.9855e-02,\n",
      "        -8.9719e-02, -8.9548e-02, -8.8365e-02, -8.8248e-02, -8.7464e-02,\n",
      "        -8.7446e-02, -8.4649e-02, -8.4634e-02, -8.4038e-02, -8.3951e-02,\n",
      "        -8.3305e-02, -8.2423e-02, -8.1981e-02, -8.1530e-02, -8.1343e-02,\n",
      "        -8.0373e-02, -8.0370e-02, -7.9749e-02, -7.8515e-02, -7.5747e-02,\n",
      "        -7.5486e-02, -7.5484e-02, -7.5476e-02, -7.4907e-02, -7.3758e-02,\n",
      "        -7.3590e-02, -7.3396e-02, -6.9953e-02, -6.8446e-02, -6.8026e-02,\n",
      "        -6.6301e-02, -6.6271e-02, -5.9923e-02, -5.7786e-02, -5.7428e-02,\n",
      "        -5.6715e-02, -5.6588e-02, -5.6530e-02, -5.4966e-02, -5.4435e-02,\n",
      "        -5.1994e-02, -5.1050e-02, -4.9782e-02, -4.9138e-02, -4.8706e-02,\n",
      "        -4.7207e-02, -4.7108e-02, -4.6207e-02, -4.6148e-02, -4.6019e-02,\n",
      "        -4.4010e-02, -4.3598e-02, -4.3405e-02, -4.2610e-02, -4.2339e-02,\n",
      "        -4.1440e-02, -4.1390e-02, -4.0913e-02, -3.9309e-02, -3.7761e-02,\n",
      "        -3.7726e-02, -3.7500e-02, -3.6939e-02, -3.6892e-02, -3.6068e-02,\n",
      "        -3.5995e-02, -3.4647e-02, -3.3434e-02, -3.2707e-02, -3.2687e-02,\n",
      "        -3.1628e-02, -2.8415e-02, -2.7061e-02, -2.6966e-02, -2.6370e-02,\n",
      "        -2.6280e-02, -2.5810e-02, -2.5800e-02, -2.3147e-02, -2.2940e-02,\n",
      "        -2.2772e-02, -1.9060e-02, -1.7234e-02, -1.7196e-02, -1.3688e-02,\n",
      "        -1.3000e-02, -1.2318e-02, -1.1557e-02, -1.1301e-02, -1.0710e-02,\n",
      "        -9.5884e-03, -9.0703e-03, -7.1802e-03, -3.3103e-03, -1.3885e-03,\n",
      "         2.4209e-04,  1.1066e-03,  1.9863e-03,  2.7722e-03,  4.7826e-03,\n",
      "         5.2774e-03,  5.6063e-03,  8.4522e-03,  8.7899e-03,  8.9078e-03,\n",
      "         1.1245e-02,  1.1333e-02,  1.2062e-02,  1.2507e-02,  1.4237e-02,\n",
      "         1.5382e-02,  1.5787e-02,  1.8016e-02,  1.8442e-02,  1.8771e-02,\n",
      "         2.3123e-02,  2.3352e-02,  2.4550e-02,  2.6159e-02,  2.7064e-02,\n",
      "         2.7092e-02,  2.7563e-02,  2.7725e-02,  2.8600e-02,  3.0232e-02,\n",
      "         3.5000e-02,  3.5051e-02,  3.7885e-02,  3.8126e-02,  3.8751e-02,\n",
      "         3.8753e-02,  3.9545e-02,  4.0380e-02,  4.0631e-02,  4.1200e-02,\n",
      "         4.2501e-02,  4.4307e-02,  4.5966e-02,  4.6412e-02,  4.7247e-02,\n",
      "         4.8071e-02,  4.8212e-02,  4.9531e-02,  5.0040e-02,  5.0994e-02,\n",
      "         5.3948e-02,  5.5639e-02,  5.6868e-02,  5.8597e-02,  5.9230e-02,\n",
      "         6.1417e-02,  6.4551e-02,  6.4691e-02,  6.6354e-02,  6.7628e-02,\n",
      "         6.8181e-02,  6.9355e-02,  7.0629e-02,  7.1767e-02,  7.2114e-02,\n",
      "         7.2615e-02,  7.4017e-02,  7.6828e-02,  7.7747e-02,  7.8173e-02,\n",
      "         7.8178e-02,  7.9252e-02,  7.9663e-02,  8.0325e-02,  8.0858e-02,\n",
      "         8.1441e-02,  8.1510e-02,  8.2597e-02,  8.3208e-02,  8.4819e-02,\n",
      "         8.5618e-02,  8.6390e-02,  8.7556e-02,  8.7884e-02,  8.9967e-02,\n",
      "         8.9995e-02,  9.3720e-02,  9.4515e-02,  9.5043e-02,  9.5174e-02,\n",
      "         9.6473e-02,  9.7529e-02,  9.7907e-02,  9.9110e-02,  9.9859e-02,\n",
      "         1.0101e-01,  1.0160e-01,  1.0433e-01,  1.0443e-01,  1.0676e-01,\n",
      "         1.0678e-01,  1.0752e-01,  1.1250e-01,  1.1285e-01,  1.1291e-01,\n",
      "         1.1396e-01,  1.1630e-01,  1.1802e-01,  1.1858e-01,  1.2111e-01,\n",
      "         1.2223e-01,  1.2277e-01,  1.2479e-01,  1.2576e-01,  1.2635e-01,\n",
      "         1.2703e-01,  1.2893e-01,  1.2898e-01,  1.3089e-01,  1.3098e-01,\n",
      "         1.3112e-01,  1.3142e-01,  1.3309e-01,  1.3317e-01,  1.3429e-01,\n",
      "         1.3507e-01,  1.3585e-01,  1.3621e-01,  1.3675e-01,  1.3723e-01,\n",
      "         1.3756e-01,  1.3828e-01,  1.3857e-01,  1.4219e-01,  1.4466e-01,\n",
      "         1.4513e-01,  1.4886e-01,  1.4976e-01,  1.5098e-01,  1.5357e-01,\n",
      "         1.5477e-01,  1.5916e-01,  1.6357e-01,  1.6563e-01,  1.6704e-01,\n",
      "         1.6761e-01,  1.6787e-01,  1.7162e-01,  1.7337e-01,  1.7436e-01,\n",
      "         1.7548e-01,  1.7578e-01,  1.7611e-01,  1.7872e-01,  1.7879e-01,\n",
      "         1.7935e-01,  1.8151e-01,  1.8182e-01,  1.8288e-01,  1.9023e-01,\n",
      "         1.9087e-01,  1.9148e-01,  1.9941e-01,  2.0072e-01,  2.0267e-01,\n",
      "         2.0499e-01,  2.0662e-01,  2.0734e-01,  2.0756e-01,  2.0774e-01,\n",
      "         2.0830e-01,  2.0934e-01,  2.1004e-01,  2.1080e-01,  2.1096e-01,\n",
      "         2.1758e-01,  2.1801e-01,  2.1952e-01,  2.2021e-01,  2.2162e-01,\n",
      "         2.2240e-01,  2.2272e-01,  2.2277e-01,  2.2277e-01,  2.2416e-01,\n",
      "         2.2420e-01,  2.2487e-01,  2.2906e-01,  2.2969e-01,  2.3194e-01,\n",
      "         2.3281e-01,  2.3625e-01,  2.3819e-01,  2.3848e-01,  2.3850e-01,\n",
      "         2.3872e-01,  2.4150e-01,  2.4279e-01,  2.4493e-01,  2.4553e-01,\n",
      "         2.4564e-01,  2.4650e-01,  2.4669e-01,  2.4780e-01,  2.4962e-01,\n",
      "         2.5065e-01,  2.5095e-01,  2.5260e-01,  2.5308e-01,  2.5361e-01,\n",
      "         2.5503e-01,  2.5687e-01,  2.6176e-01,  2.6238e-01,  2.6419e-01,\n",
      "         2.6430e-01,  2.6534e-01,  2.6829e-01,  2.6842e-01,  2.6849e-01,\n",
      "         2.6878e-01,  2.6909e-01,  2.7203e-01,  2.7473e-01,  2.7648e-01,\n",
      "         2.7822e-01,  2.7979e-01,  2.8152e-01,  2.8254e-01,  2.8581e-01,\n",
      "         2.8641e-01,  2.8733e-01,  2.9036e-01,  2.9045e-01,  2.9099e-01,\n",
      "         2.9122e-01,  2.9229e-01,  2.9272e-01,  2.9413e-01,  2.9710e-01,\n",
      "         2.9754e-01,  2.9886e-01,  3.0111e-01,  3.0147e-01,  3.0625e-01,\n",
      "         3.0677e-01,  3.0768e-01,  3.1312e-01,  3.1541e-01,  3.1586e-01,\n",
      "         3.1838e-01,  3.1921e-01,  3.2072e-01,  3.2403e-01,  3.2637e-01,\n",
      "         3.3040e-01,  3.3044e-01,  3.3361e-01,  3.3433e-01,  3.3439e-01,\n",
      "         3.3999e-01,  3.4351e-01,  3.4413e-01,  3.4505e-01,  3.4610e-01,\n",
      "         3.5028e-01,  3.5042e-01,  3.5096e-01,  3.5144e-01,  3.5198e-01,\n",
      "         3.5299e-01,  3.5306e-01,  3.5327e-01,  3.5545e-01,  3.6584e-01,\n",
      "         3.6709e-01,  3.6915e-01,  3.7592e-01,  3.7660e-01,  3.7723e-01,\n",
      "         3.7865e-01,  3.8021e-01,  3.8069e-01,  3.8085e-01,  3.8246e-01,\n",
      "         3.8920e-01,  3.9136e-01,  3.9645e-01,  3.9771e-01,  3.9832e-01,\n",
      "         3.9908e-01,  4.0242e-01,  4.0425e-01,  4.0808e-01,  4.1430e-01,\n",
      "         4.1463e-01,  4.1512e-01,  4.1623e-01,  4.1904e-01,  4.2025e-01,\n",
      "         4.2118e-01,  4.2351e-01,  4.2483e-01,  4.3216e-01,  4.3403e-01,\n",
      "         4.4119e-01,  4.4154e-01,  4.4484e-01,  4.5279e-01,  4.5431e-01,\n",
      "         4.6163e-01,  4.6428e-01,  4.6635e-01,  4.6876e-01,  4.6956e-01,\n",
      "         4.7087e-01,  4.7203e-01,  4.8296e-01,  4.8313e-01,  4.8754e-01,\n",
      "         4.9155e-01,  4.9950e-01,  5.0103e-01,  5.0718e-01,  5.0964e-01,\n",
      "         5.1406e-01,  5.1497e-01,  5.3144e-01,  5.3154e-01,  5.3407e-01,\n",
      "         5.3450e-01,  5.3746e-01,  5.4119e-01,  5.4520e-01,  5.4632e-01,\n",
      "         5.4876e-01,  5.6415e-01,  5.6553e-01,  5.7888e-01,  5.9895e-01,\n",
      "         6.1075e-01,  6.2824e-01,  6.3485e-01,  6.3542e-01,  6.5733e-01,\n",
      "         6.8009e-01,  6.8122e-01,  6.8982e-01,  6.9369e-01,  7.0276e-01,\n",
      "         7.3347e-01,  7.5025e-01,  7.7096e-01,  7.9627e-01,  8.0832e-01,\n",
      "         8.3110e-01,  8.7570e-01,  9.1205e-01,  9.4501e-01,  9.6299e-01,\n",
      "         9.8130e-01,  9.9888e-01,  1.0299e+00,  1.0715e+00,  1.1381e+00,\n",
      "         1.5650e+00,  1.6045e+00,  1.6571e+00,  2.3970e+00,  1.2265e+01,\n",
      "         9.0851e+01,  1.0455e+02,  2.3862e+02]),\n",
      "indices=tensor([314, 374, 442, 627, 365, 167, 554, 102, 503, 195, 526,  19, 459, 511,\n",
      "        373,  64, 623, 706, 266,  87, 756, 660, 480, 256, 709, 680, 396,  47,\n",
      "        566, 581, 670, 260, 642, 164, 271, 155, 725, 574, 548, 544, 540,  95,\n",
      "        470, 132, 324, 498,   9, 110, 318, 251,   5, 745, 506, 439, 339, 608,\n",
      "        514,  61, 655, 409, 596,  27, 201,  41, 222, 547, 269, 589, 191,  67,\n",
      "        476, 456, 234,  88, 568, 300, 227, 290, 186, 599, 751,  66, 724, 204,\n",
      "        569,  31, 184, 723, 153, 200, 562, 764, 141, 563, 537, 702, 342, 766,\n",
      "        299, 557,  82, 288, 361, 600, 479,  35,  80, 645, 347, 650, 471, 490,\n",
      "        418, 169, 452, 254, 553,  92,  53, 183, 382, 295,   3, 399,  60, 635,\n",
      "        139, 431, 502, 686, 495, 729,  56, 412, 649, 150, 208, 415,  33,  98,\n",
      "        578, 499, 359, 567, 309, 175, 144, 126, 691, 117, 579, 733, 219,  16,\n",
      "        469, 170, 162, 500, 719,  40, 333, 134, 561, 549, 127, 672, 648, 197,\n",
      "        353, 337, 118, 329, 533, 705,  13, 247, 461, 575, 493, 536, 123,  12,\n",
      "        436, 516, 303, 109,  32, 424, 335, 613, 765, 677, 504, 380, 332, 441,\n",
      "        413, 159, 328, 281, 564, 226, 532, 590, 371, 595, 586,  38, 638, 462,\n",
      "        588, 422, 136,   1, 343, 320,  63, 427, 545, 289, 257, 312, 755, 722,\n",
      "        618, 538, 357, 230, 698, 236, 245, 717, 416, 757, 654,  54, 647, 631,\n",
      "        171, 610,  62, 726, 292, 129, 321, 419,  97, 156, 218, 443, 624, 530,\n",
      "        112, 148,  18, 235, 403,  22, 612, 115,  30, 699,  79, 135,  17, 609,\n",
      "        229, 319, 632, 483, 748, 515, 734,  23, 317, 212, 475,   8,  94, 542,\n",
      "        375, 291, 651, 133, 174, 468, 192,  44, 121,  96, 243, 355, 188, 244,\n",
      "        664, 583, 104,  10, 311, 634, 249,  59,  14,  46, 100, 440, 678, 206,\n",
      "        643,  74, 388, 398, 508, 674, 414, 448, 330, 693, 345, 376, 445, 395,\n",
      "        344,  71, 181, 519, 606, 523, 196, 629, 582, 718,  21, 558, 694, 310,\n",
      "         86,  26,  15,  85, 238,  39, 402, 491, 644,  77, 707, 228, 689, 619,\n",
      "        673,   2, 701, 190, 614, 657, 453, 223, 296, 366, 338, 690, 390, 143,\n",
      "        492, 444, 165, 598, 203,  75, 157, 607, 710, 323, 301, 531, 704, 527,\n",
      "        426, 433, 659, 154, 274, 752, 106, 636, 393, 467,  73, 216, 429, 225,\n",
      "        428, 131, 454, 465, 708, 114,  50, 555, 518, 307, 213, 472, 737, 270,\n",
      "        747, 641, 182, 510, 128, 240, 559, 615,   0, 620, 742, 263, 528, 732,\n",
      "        662, 334, 621, 327, 385, 758, 172, 325, 404, 594, 246, 534, 762, 358,\n",
      "        663, 521, 105, 653, 543, 488, 370,  45, 410, 524, 682, 122, 687, 466,\n",
      "          7,  51, 735, 501, 287, 587, 754, 685, 721, 198, 712, 239, 437, 681,\n",
      "        576, 124, 529, 665, 571, 713, 283, 211, 556, 736, 749, 715, 435,  78,\n",
      "        378, 580,  70, 616, 147, 746,  28,  72, 675, 392, 214, 302, 550, 669,\n",
      "        741, 340, 160,  68, 656, 727, 546, 700, 252, 605, 551, 640, 420, 161,\n",
      "        273,  99, 714, 486, 535, 383, 716, 667, 336, 509, 298, 464, 341, 460,\n",
      "        194, 261, 268,  24, 630, 512, 601, 597, 487, 637, 451, 692, 411, 446,\n",
      "        591, 484, 552, 455, 572,  90, 565, 625, 405, 585, 485, 695, 761, 602,\n",
      "        125, 202,  58, 111, 264, 316, 671,  76, 617, 293, 348,  25, 231, 278,\n",
      "        272, 730, 140, 237, 666, 163, 101, 584,  37, 611, 130,  42, 573, 360,\n",
      "        760, 696, 684, 457, 356,  11, 187, 221, 407, 697, 577, 180, 505, 513,\n",
      "        425,  57, 265,  43, 207, 233, 482, 250, 137, 406, 740, 346, 626, 113,\n",
      "        210, 628, 308, 215, 315,  55, 220, 352, 400, 189, 158, 363, 116, 604,\n",
      "         83, 152, 350, 178, 313, 688, 284, 603, 372, 384,  65, 349,  81, 120,\n",
      "        417, 646, 397, 305, 176, 438, 458,  84, 279, 522, 369, 463, 285, 282,\n",
      "        367, 368, 432, 421, 173, 750, 477, 151,  20, 168, 177, 280, 658, 711,\n",
      "        560,  91, 276, 262, 179, 297, 676, 354, 394, 386, 744, 381, 205, 391,\n",
      "        199, 661,  69, 520, 753, 494, 275,  89, 652, 478, 241, 306, 541, 387,\n",
      "        277, 767, 592, 743, 217, 668, 759, 242, 362, 146, 248, 364, 507, 763,\n",
      "        497, 142, 731,  93, 738, 286, 224, 633, 294, 209,  29,  49, 703, 423,\n",
      "        326, 728, 108, 149, 185, 539, 322,   4, 474, 639, 408, 679, 259, 253,\n",
      "        255,  48, 434, 351, 517, 489, 193, 138, 379, 593, 331, 304, 267, 401,\n",
      "        739,  34, 570, 119, 258, 473, 683,  52, 450, 166, 622, 103, 377, 525,\n",
      "        447, 449, 389, 481, 145, 232, 107, 720,   6,  36, 430, 496]))\n"
     ]
    }
   ],
   "source": [
    "# TODO: Data whitening\n",
    "# Perform only if needed later\n",
    "print(activations[0].sort())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "520f3f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for act in activations[:10]:\n",
    "    actNormed = model.transformer.ln_f(act).detach().numpy()\n",
    "    # layerNorm = torch.nn.LayerNorm(act.size(-1), elementwise_affine=False).to(device).eval()\n",
    "    # actNormed = layerNorm(act).cpu().detach().numpy()\n",
    "    # print(torch.allclose(act, actNormed)) #<- Outputs False, so layerNorm *should* be doing something\n",
    "    \n",
    "    \n",
    "    (neuronActivations, indices) = zip(*enumerate(actNormed))\n",
    "    plt.bar(range(len(act)), act, width=10)\n",
    "plt.xlabel(\"Indices\")\n",
    "plt.ylabel(\"Neuron Activations\")\n",
    "plt.show()\n",
    "# What's happening around neurons 400-500 and ~300???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3cf401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the magic to make sense of the interesting bits\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, inDims, codeDims, tied=False, topk=None):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(inDims, codeDims, bias=True) # Bias is true because it's good at learning activation offsets\n",
    "        self.decoder = nn.Linear(codeDims, inDims, bias=False) # Bias is false because we want representations to be just combinations of feature directions\n",
    "        # Note: If we had bias true, some feature vectors could be offset by an arbitrary amount,\n",
    "        # making it harder to compare veature vectors in some situations\n",
    "        self.tied = tied # True can help improve improve how identifiable features are, constrains the solution\n",
    "        self.topk = topk # Hard sparsity - keeps only largest k activations - used for fixed amount of active features\n",
    "        if tied:\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "    def encode(self, x):\n",
    "        # x is the hidden layer we're passing in\n",
    "        s = self.encoder(x)\n",
    "        if self.topk is not None:\n",
    "            # k = self.topk\n",
    "            # Grab the top k values and their dimensions\n",
    "            topkVals, topkIndex = torch.topk(s, self.topk, dim=-1)\n",
    "            # Make a mask out of them \n",
    "            mask = torch.zeros_like(s).scatter_(-1, topkIndex, 1.0)\n",
    "            # Multiply the mask in and kill any features that aren't within the {self.topk} dimensions\n",
    "            # Works as a \"hard sparsity\"\n",
    "            s = s * mask\n",
    "            s = torchFun.relu(s)\n",
    "        else:\n",
    "            # Relies on L1 penalty in loss - \"soft sparsity\"\n",
    "            s = torchFun.relu(s)\n",
    "        return s\n",
    "    \n",
    "    def forward(self, x):\n",
    "        s = self.encode(x)\n",
    "        xHat = self.decoder(s)\n",
    "        return xHat, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75be99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAE training function\n",
    "import tqdm\n",
    "\n",
    "def train_sae(X, codeDims=4*modelDims, l1=1e-3, epochs=5, batchSize=256, topk=None, learningRate=1e-3, tied=False):\n",
    "    sae = SAE(inDims=modelDims, codeDims=codeDims, tied=tied, topk=topk).to(device)\n",
    "    optimizer = torch.optim.Adam(sae.parameters(), lr=learningRate)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X) # X is CPU tensor\n",
    "    dataLoader = DataLoader(dataset, batch_size=batchSize, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "    for ep in tqdm(range(epochs)):\n",
    "        losses, reconstructionLosses, l1_losses = [], [], []\n",
    "        for (batchActivations,) in dataLoader:\n",
    "            batchActivations = batchActivations.to(device)\n",
    "            xhat, s = sae(batchActivations)\n",
    "            reconstructionLoss = torchFun.mse_loss(xhat, batchActivations)\n",
    "            l1Penalty = s.abs().mean()\n",
    "            loss = reconstructionLoss + l1 * l1Penalty\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            reconstructionLosses.append(reconstructionLoss.item())\n",
    "            l1_losses.append(l1Penalty.item())\n",
    "        t.set_description(f\"ep {ep}: loss {sum(losses)/len(losses):.4f} | recon {sum(recon_losses)/len(recon_losses):.4f} | L1 {sum(l1_losses)/len(l1_losses):.4f}\")\n",
    "        t.refresh()\n",
    "    \n",
    "    return sae.eval()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc9d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helpful function to get the topk features\n",
    "# This is what maximally activates features\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_feature_topk(sae, X, k=20, batch=1024):\n",
    "    indexes = []\n",
    "    allScores = []\n",
    "    for i in range(0, X.size(0), batch):\n",
    "        batchActivations = X[i:i+batch].to(device) # xb is a batch of activations\n",
    "        _, s = sae(batchActivations) # s is sparse code\n",
    "        allScores.append(s.detach().cpu())\n",
    "    scores = torch.cat(allScores, dim=0)\n",
    "\n",
    "    for j in range(scores.size(1)):\n",
    "        vals, topIndex = torch.topk(scores[:, j], k)\n",
    "        indexes.append(topIndex)\n",
    "    return indexes, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd0f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature steering\n",
    "# Lets us push features in directions (small + 1 = medium + 1 = large)\n",
    "# Shows that a direction has meaning\n",
    "\n",
    "@torch.no_grad()\n",
    "def steer_with_feature(model, tokenizer, prompt, sae, featureId, epsilon=0.5):\n",
    "    # Run a pass through the LM\n",
    "    encoder = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    out = model(**encoder, output_hidden_states=True)\n",
    "    H = out.hidden_states[-1][:, -1, :]\n",
    "\n",
    "    # Run a pass through the SAE with the LM's hidden states\n",
    "    _, s = sae(H)\n",
    "    d = sae.decoder.weight[featureId]\n",
    "\n",
    "    # Steer the SAE's outputs in a direction\n",
    "    H_steered = H + epsilon * d\n",
    "\n",
    "    # Pass the logits back out for analysis\n",
    "    logits = model.lm_head(H)\n",
    "    logits_steered = model.lm_head(H_steered)\n",
    "    return logits.squeeze(0), logits_steered.squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c0efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stability across restarts\n",
    "@torch.no_grad()\n",
    "def match_features_by_cosine(D1, D2):\n",
    "    # D1, D2: (d_code, d_model) row-major weights\n",
    "    #Normalize rows\n",
    "    D1n = torchFun.normalize(D1, dim1)\n",
    "    D2n = torchFun.normalize(D2, dim1)\n",
    "    sim = D1n @ D2n.T\n",
    "\n",
    "    # Basic argmax\n",
    "    row_best = sim.max(dim=1).values.mean().item()\n",
    "    col_best = sim.max(dim=0).values.mean().item()\n",
    "    return sim, row_best, col_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe6e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions for analysis\n",
    "@torch.no_grad()\n",
    "def eval_reconstruction_stats(sae, Xeval, batch=2048):\n",
    "  reconstructionSum, elements = 0.0, 0\n",
    "  allScores = []\n",
    "  for i in range(0, Xeval, batch):\n",
    "    xb = Xeval[i:i+batch].to(device)\n",
    "    xhat, score = sae(xb)\n",
    "    # Calculate the mean squared error of the model over the evaluation ds\n",
    "    reconstructionSum += torchFun.mse_loss(xhat, xb, reduction='sum').item()\n",
    "    elements += xb.numel()\n",
    "    allScores.append(score.detach().cpu())\n",
    "  mse = reconstructionSum / elements\n",
    "  variance = Xeval.pow(2).mean().item()\n",
    "  r2 = max(0.0, 1.0 - mse / (variance + 1e-12))\n",
    "  score = torch.cat(allScores, dim=0)\n",
    "  return mse, r2, score\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_sparsity_metrics(S):\n",
    "  density = (S > 0).float().mean().item()\n",
    "  deadRate = ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MATS-9.0 (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
